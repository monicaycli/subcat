---
title: "GAMM"
author: "Monica Li"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    theme: united
    highlight: pygments
    df_print: paged
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE, cache=FALSE}
# knitr chunk settings
knitr::opts_chunk$set(cache = TRUE,
                      cache.lazy = FALSE,
                      cache.comments = FALSE,
                      autodep = TRUE,
                      echo = FALSE)
knitr::dep_auto()

# knitr format setting
options(knitr.table.format = "html")

# import functions
library(Hmisc)
library(MASS)
library(car)
library(knitr)
library(ggplot2)
library(reshape)
library(plyr)
library(grid)
library(gridExtra)
library(cluster)
library(lme4)
library(kableExtra)
library(tidyr)
library(magrittr)
library(VWPre)
library(mgcv)
library(itsadug)
library(mice)

library(fs)
library(here)

source(fs::path(here::here(), "ANALYSIS", "et_tools_subcat.r"))
```

```{r env info, include=TRUE}
devtools::session_info()
```

```{r import individual differences data, echo=FALSE, include=FALSE}
# read in data
test_scores <- read.csv(fs::path(here::here(), "DATA", "TEST_SCORES", ext = "csv"), header = T)

# remove invalid data point(s)
test_scores$towre.nw.time[test_scores$towre.nw.time > 45] <- NA

# unfactor certain columns
test_scores$gort.fluen <- as.numeric(as.character(test_scores$gort.fluen))
test_scores$gort.wpm   <- as.numeric(as.character(test_scores$gort.wpm))
test_scores$gort.time  <- as.numeric(as.character(test_scores$gort.time))

# re-calculate some scores
# items per minute
test_scores$towre.w.ipm <- with(test_scores, (towre.w.acc/towre.w.time)*60)
test_scores$towre.nw.ipm <- with(test_scores, (towre.nw.acc/towre.nw.time)*60)

# set min values for print experience (authors and magazines) as 0
test_scores$art[test_scores$art < 0] <- 0
test_scores$mrt[test_scores$mrt < 0] <- 0

# add Gates-MacGinitie grade equivalent scores
gates.table <- read.csv(fs::path(here::here(), "DATA", "GATES_CONVERSION", ext = "csv"), header = T)
gates.table %<>% 
  plyr::rename(c(Raw.Score = "gm.rcomp.raw", GE = "gm.rcomp.grade")) %>%
  subset(select = c(gm.rcomp.raw, gm.rcomp.grade))

test_scores$gm.rcomp.grade <-
  gates.table$gm.rcomp.grade[match(test_scores$gm.rcomp.raw,
                                   gates.table$gm.rcomp.raw)]
```

```{r categorize ID measures, echo=FALSE, fig.width=7, fig.height=7}
# including
vrcomp <- c("gm.rcomp.raw","piat.r.raw","fr.raw","wj3.rcomp.raw")
vlcomp <- c("piat.l.raw","wj3.oralcomp.raw")
vvocab <- c("ppvt.raw","wasi.vocab.raw")
vdecode <- c("towre.w.acc","wj3.wid.raw","towre.nw.acc","wj3.watt.raw")
vfluen <- c("gort.fluen","wj3.rf.raw")
vran <- c("ctopp.rcn","ctopp.rdn","ctopp.rln")
vphono <- c("ctopp.blending","ctopp.elision","ctopp.dspan","ctopp.nwrep")
vsspan <- c("sspan2.raw")
vprint <- c("art","mrt")
vgeneral <- c("wasi.matr.raw","corsi","wasi.iq")
vdemo <- c("age.dec","edu.years","ses.2.status")
vars <- c(vrcomp, vlcomp, vvocab, vdecode, vfluen, vran, vphono, vsspan, vprint, vgeneral, vdemo)
vgrade <- c("gm.rcomp.grade","piat.r.grade","wj3.rcomp.grade",
            "piat.l.grade","wj3.oralcomp.grade",
            "wj3.wid.grade","wj3.watt.grade","wj3.rf.grade")
```

```{r descriptive stats of ID measures}
# select measures of interest
data_ids_w_grade <- test_scores[c("subj", vars, vgrade)]

# exclude subjects
data_ids_w_grade %<>%
  subset(!(subj %in% c(126, 133, 138, 175)))
```

```{r calculate box-cox lambda for ID measures, echo=FALSE}
data <- data_ids_w_grade[c("subj", vars)]

bestLambda <- list()
for (i in seq(ncol(data))) {
  if (0 %in% data[[i]]) {
    bc <- MASS::boxcox(lm(data[[i]]+mean(data[[i]])~1), plotit = F)
  } else {
    bc <- MASS::boxcox(lm(data[[i]]~1), plotit = F)
  }
  bestLambda <- cbind(bestLambda, bc$x[which.max(bc$y)])
}

t_bestLambda <- data.frame(ID = names(data), lambda = unlist(t(bestLambda)))
```

```{r multiple imputation for missing data in ID measures, include=FALSE}
# check to see missing data don't exceed 5%
pMiss <- function(x){sum(is.na(x))/length(x)*100}
apply(data, 2, pMiss)
apply(data, 1, pMiss)

# check missing data pattern
mice::md.pattern(data, plot = FALSE)

# run imputation
tempData <- mice::mice(data, m = 5, maxit = 50, method = 'pmm', seed = 500,
                       printFlag = FALSE)
summary(tempData)

# Replace missing values with the imputed values in the first of the five datasets.
completedData <- mice::complete(tempData, 1)
```

```{r invert variables, include=FALSE}
# make a copy of the imputated data
# transform measures where higher scores indicate poorer performance by subtracting individual scores from their correspoding maximum observed scores
data_trans <- completedData
data_trans$ctopp.rcn <- max(data_trans$ctopp.rcn) - data_trans$ctopp.rcn
data_trans$ctopp.rdn <- max(data_trans$ctopp.rdn) - data_trans$ctopp.rdn
data_trans$ctopp.rln <- max(data_trans$ctopp.rln) - data_trans$ctopp.rln
```

```{r box-cox transformation and standardization, include=FALSE}
# BOX-COX TRANSFORMATION & STANDARDIZATION
data <- data_trans

# find the best lambda for each variable
bestLambda <- list()
for (i in seq(ncol(data))) {
  if (0 %in% data[[i]]) {
    bc <- MASS::boxcox(lm(data[[i]]+mean(data[[i]])~1), plotit = F)
  } else {
    bc <- MASS::boxcox(lm(data[[i]]~1), plotit = F)
  }
  bestLambda <- cbind(bestLambda, bc$x[which.max(bc$y)])
}

# boxcox transformation
data.bc <- data
for (i in seq(ncol(data.bc))) {
  if (0 %in% data.bc[[i]]) {
    data.bc[[i]] <- data.bc[[i]] + mean(data.bc[[i]])
  }
  data.bc[[i]] <- car::bcPower(data.bc[[i]], bestLambda[[i]])
}

# standardization  
data.bc.st <- apply(data.bc, 2, function(x) (x - mean(x))/sd(x))
```

```{r create and standardize composites}
data_composite <- data.frame(matrix(ncol = 11, nrow = 60))
colnames(data_composite) <- c("Reading Comprehension",
                              "Oral Comprehension & Vocabulary",
                              "Decoding",
                              "Fluency",
                              "RAN",
                              "Phonological Skills",
                              "Verbal Working Memory",
                              "Print Experience",
                              "Matrix Reasoning",
                              "Visuospatial Memory",
                              "IQ")
# average variables
data_composite$`Reading Comprehension`            <- rowMeans(data.bc.st[,vrcomp])
data_composite$`Oral Comprehension & Vocabulary`  <- rowMeans(data.bc.st[,c(vlcomp,vvocab)])
data_composite$Decoding                           <- rowMeans(data.bc.st[,vdecode])
data_composite$Fluency                            <- rowMeans(data.bc.st[,vfluen])
data_composite$RAN                                <- rowMeans(data.bc.st[,vran])
data_composite$`Phonological Skills`              <- rowMeans(data.bc.st[,vphono])
data_composite$`Verbal Working Memory`            <- data.bc.st[,vsspan]
data_composite$`Print Experience`                 <- rowMeans(data.bc.st[,vprint])
data_composite$`Matrix Reasoning`                 <- data.bc.st[,"wasi.matr.raw"]
data_composite$`Visuospatial Memory`              <- data.bc.st[,"corsi"]
data_composite$IQ                                 <- data.bc.st[,"wasi.iq"]

# standardize composite scores
data_composite.st <- data.frame(apply(data_composite, 2, function(x) (x - mean(x))/sd(x)))
```

```{r grouping participants into tertiles based on composite scores, echo=FALSE}
cat_idx <- data.frame(subj = data$subj, 
                      phono.composite = data_composite.st$Phonological.Skills)

cat_idx$groups.phono.3 <- cut(cat_idx$phono.composite, 
                      breaks = quantile(cat_idx$phono.composite,
                                        probs = seq(0, 1, 1/3)),
                      include.lowest = TRUE,
                      right = FALSE)
levels(cat_idx$groups.phono.3) <- c("low (0-33.33%)","mid (33.33-66.67%)","high (66.67-100%)")
```

```{r subcat trial abortion info}
Acc <- read.csv(fs::path(here::here(), "DATA", "TRIAL_INFO", ext = "csv"), header = T)

Acc_rm <-
  Acc %>%
  dplyr::mutate(., trialid = trialid + 1) %>%
  dplyr::mutate(., subj = (subj %>% factor)) %>%
  subset(!(subj %in% c("126", "133", "138", "175"))) %>%
  droplevels()
```

```{r load eyetracking data, echo=FALSE, include=FALSE}
filepath <- fs::path(here::here(), "DATA", "SUBCAT_ET_DATA", ext = "csv")
dat <- readFixationReport(filepath, sep = ',', screenSize = c(1024,768))

# change subject id format
dat$SUBJECT <- factor(dat$SUBJECT)

# merge accuracy info and eyetracking data, remove subjects, remove aborted trials
dat <-
  merge(dat, Acc_rm,
        by.x = c("SUBJECT","TRIAL"),
        by.y = c("subj","trialid")) %>%
  subset(aborted == FALSE) %>%
  droplevels()

# rearrange level order for COND
dat$COND <- factor(dat$COND, levels = c("W1W1", "W2W1", "N3W1", "Filler"))

# CROSS PADDING
# sort the dataframe by SUBJECT, TRIAL, and CURRENT_FIX_START
dat <- dat[with(dat,order(SUBJECT,TRIAL,CURRENT_FIX_START)),]
# extract the last row of each trial from each subject
lastrow <- dat[!duplicated(dat[c("SUBJECT","TRIAL")],fromLast = TRUE),]
# change the start time (to the end time of the last fixation point)
lastrow$CURRENT_FIX_START <- lastrow$CURRENT_FIX_END
# change the end time (to match the fixation end time of the longest trial)
lastrow$CURRENT_FIX_END <- max(lastrow$CURRENT_FIX_END)
# change the fixation duration
lastrow$CURRENT_FIX_DURATION <- lastrow$CURRENT_FIX_END - lastrow$CURRENT_FIX_START
# change the fixation location attributes (as if looking at center)
lastrow$CURRENT_FIX_X <- 0
lastrow$CURRENT_FIX_Y <- 0
lastrow$FIXATED_CELL <- 13
lastrow$FIXATED_LOCATION <- "CTR"
lastrow$FIXATED_OBJECT <- "ctr"

# combine the last rows back to the original dataset
dat <- rbind(dat,lastrow)
# sort data based on SUBJECT, TRIAL, and CURRENT_FIX_START
dat <- dat[with(dat,order(SUBJECT,TRIAL,CURRENT_FIX_START)),]

# CODE ITEM TYPE (SIMILAR vs DISTINCT PLACE OF ARTICULATION)
### code filler trials as 0
### code similar as 1, distinct as 2
dat$POA_CODE <- ifelse(dat$COND == "Filler",0,
                       ifelse(dat$TARGET %in% c("carp","harp","cat","road","knot","beak"),2,1))
```

```{r fix prop, echo=FALSE, fig.width=15, fig.height=10}
gaze.IA <- assignIA(dat)
gazeBins_2000 <- binifyFixations(gaze.IA, binSize = 50, maxTime = 2000,
                                 keepCols = c("SUBJECT", "COND", "TARGET","TRIAL", 
                                            "T", "C", "D", "X", "O", "POA_CODE"))

gazeSubj <- computeFixProp(gazeBins_2000)
gazeSubj.s <- subset(gazeSubj, (COND != "Filler" & Time >= 0))
gazeSubj.s$FIX_OBJECT <- factor(gazeSubj.s$FIX_OBJECT, levels = c("T","C","D","X","O"))
levels(gazeSubj.s$FIX_OBJECT) <- c("target","competitor","distractor","cross","other")

# add subject grouping to the eyetracking data.frame
gazeSubj.s <- merge(gazeSubj.s, cat_idx, by.x = "SUBJECT", by.y = "subj", sort = T)
# sort data frame
gazeSubj.s <- gazeSubj.s[with(gazeSubj.s, order(SUBJECT, COND, FIX_OBJECT, Time)),]

# reverse factor level order
gazeSubj.s$groups.phono.3 <- 
  with(gazeSubj.s, factor(groups.phono.3, levels = rev(levels(groups.phono.3))))
```

# GAMM on Target Fixation Proportions
* everything up until model specification remains the same as in the old analysis pipeline (i.e., preprocessing individual difference measaures, creating composite scores, calculating fixation proportions for each object and time bin)
    * phonological skills composite consists of `ctopp.blending`, `ctopp.elision`, `ctopp.dspan`, and `ctopp.nwrep`
* eyetracking analysis
    * time frame: 600 ~ 1200 ms after critical word onset
    * object: target
    * use functions from `VWPre` (preprocessing), `mgcv` (model fitting), `itsadug` (visualization)
```{r select analysis time frame, echo=FALSE}
data_600to1200 <- droplevels(subset(gazeSubj.s, Time >= 600 & Time <= 1200))
data.trg.allCon <- droplevels(subset(data_600to1200, FIX_OBJECT == "target"))
```

## 1. Preprocessing
* turn proportions into empirical logits
* mark time series onset time
* set the reference level for Condition and specify contrasts
```{r GAMM preprocessing, echo=TRUE}
# transform proportions to elogits (cf. VWPre::transform_to_elogit)
elogit = function(proportion=proportion, observations=observations, constant=constant) {
    return(log((proportion * observations + constant)/((1 - proportion) * observations + constant)))
  }
weight = function(proportion=proportion, observations=observations, constant=constant) {
    return((1/(proportion * observations + constant)) + (1/((1 - proportion) * observations + constant)))
}

Constant = 0.5 # default
ObsPerBin = 12.5 # samples per bin (250 Hz * 0.05 second)

data.trg.allCon$elogit <- elogit(proportion = data.trg.allCon$meanFix,
                                 observations = ObsPerBin,
                                 constant = Constant)
data.trg.allCon$weight <- weight(proportion = data.trg.allCon$meanFix,
                                 observations = ObsPerBin,
                                 constant = Constant)
```

```{r GAMM model prep, echo=TRUE}
# attach start point of each unique time series to the data.frame
# the first time bin of a time series is indicated as `TRUE`, otherwise `FALSE`
data.trg.allCon.start_event <- start_event(data.trg.allCon, 
                                           column = "Time", 
                                           event = c("SUBJECT","COND"),
                                           label.event = "Event")

# adjust factor levels: N3W1 < W2W1 < W1W1 (where N3W1 is the reference level)
data.trg.allCon.start_event$OFCOND <- factor(data.trg.allCon.start_event$COND, 
                                             levels = c("N3W1","W2W1","W1W1"))
data.trg.allCon.start_event$OFCOND <- as.ordered(data.trg.allCon.start_event$OFCOND)
contrasts(data.trg.allCon.start_event$OFCOND) <- 'contr.treatment'
contrasts(data.trg.allCon.start_event$OFCOND)
```

## 2. Base Model
* overall, there's a significant difference of intercept between W1W1 and N3W1 (t = 5.94, p < .001) but no between W2W1 and N3W1 (t = -1.21, p = .23)
* the curves of W2W1 and W1W1 over time are both significantly different than that of N3W1 (F = 15.46 and F = 22.35, respectively)
* random effects (interactions between Subject and Condition & between Subject and Time) are both significant, indicating strong effect of individual differences
```{r GAMM base model, echo=TRUE, fig.show='hold', warning=FALSE}
Sys.time()

# base model (without correcting for autoregression)
gamm.base <- bam(elogit ~ OFCOND
                 + s(Time)
                 + s(Time, by = OFCOND)
                 + s(Time, SUBJECT, by = COND, bs = "fs", m = 1), # random smooths
                 data = data.trg.allCon.start_event,
                 method = "ML", # maximum likelihood estimation
                 weights = 1/weight)

Sys.time()

# base model with AR1 correction
gamm.base.AR1 <- bam(elogit ~ OFCOND
                     + s(Time)
                     + s(Time, by = OFCOND)
                     + s(Time, SUBJECT, by = COND, bs = "fs", m = 1), # random smooths
                     data = data.trg.allCon.start_event,
                     method = "ML", # maximum likelihood estimation
                     weights = 1/weight,
                     AR.start = start.event, 
                     rho = itsadug::start_value_rho(gamm.base))

Sys.time()

# bs="tp". These are low rank isotropic smoothers of any number of covariates.
# By isotropic is meant that rotation of the covariate co-ordinate system will
# not change the result of smoothing. By low rank is meant that they have far
# fewer coefficients than there are data to smooth. They are reduced rank
# versions of the thin plate splines and use the thin plate spline penalty. They
# are the default smooth for s terms because there is a defined sense in which
# they are the optimal smoother of any given basis dimension/rank (Wood, 2003).
# Thin plate regression splines do not have ‘knots’ (at least not in any
# conventional sense): a truncated eigen-decomposition is used to achieve the
# rank reduction. See tprs for further details.
# bs="ts" is as "tp" but with a modification to the smoothing penalty, so that
# the null space is also penalized slightly and the whole term can therefore be
# shrunk to zero.
# bs="fs" Smooth factor interactions are often produced using by variables (see
# gam.models), but a special smoother class (see factor.smooth.interaction) is
# available for the case in which a smooth is required at each of a large number
# of factor levels (for example a smooth for each patient in a study), and each
# smooth should have the same smoothing parameter. The "fs" smoothers are set up
# to be efficient when used with gamm, and have penalties on each null sapce
# component (i.e. they are fully ‘random effects’).
# bs="re". These are parametric terms penalized by a ridge penalty (i.e. the
# identity matrix). When such a smooth has multiple arguments then it represents
# the parametric interaction of these arguments, with the coefficients penalized
# by a ridge penalty. The ridge penalty is equivalent to an assumption that the
# coefficients are i.i.d. normal random effects. See
# smooth.construct.re.smooth.spec.
```

```{r gamm.base.AR1 table}
# model summary
summary(gamm.base.AR1)

gamm.base.AR1.summary <- summary(gamm.base.AR1)
data.frame(gamm.base.AR1.summary$p.table)
data.frame(gamm.base.AR1.summary$s.table)
```

### 2.1. Model Comparison (w/ and w/o AR1 Correction)
* including AR1 correction improves model fit
```{r model comparison gamm.base and gamm.base.AR1, echo=FALSE, fig.show='hold'}
# model comparison between gamm.base and gamm.base.AR1
itsadug::acf_resid(gamm.base,
                   split_pred = c("SUBJECT","COND"),
                   main = "Base Model w/o AR1 Correction",
                   plot = TRUE)
itsadug::acf_resid(gamm.base.AR1,
                   split_pred = c("SUBJECT","COND"),
                   main = "Base Model w/ AR1 Correction",
                   plot = TRUE)
gamm.base.compare <- itsadug::compareML(gamm.base, gamm.base.AR1)
gamm.base.compare$table
```

## 3. Model w/ Phonological Skills Composite as a Fixed Effect
* significant interactions between Phono and W1W1-N3W1 (F = 4.32, p = .04) and between Phono and W2W1-N3W1 (F = 4.29, p = .006)
* no significant interaction between Time and Phono
```{r GAMM models with phonological skills, echo=TRUE, fig.show='hold', warning=FALSE}
Sys.time()

gamm.phono <- bam(elogit ~ OFCOND
                  # reference curves:
                  + s(Time) + s(phono.composite) 
                  # difference curves:
                  + s(Time, by = OFCOND) + s(phono.composite, by = OFCOND) 
                  # reference surface:
                  + ti(Time, phono.composite)
                  # difference surface:
                  + ti(Time, phono.composite, by = OFCOND)
                  # random effects:
                  + s(Time, SUBJECT, by = COND, bs = "fs", m = 1),
                  data = data.trg.allCon.start_event,
                  method = "ML", # maximum likelihood estimation
                  weights = 1/weight)

Sys.time()

gamm.phono.AR1 <- bam(elogit ~ OFCOND
                      # reference curves:
                      + s(Time) + s(phono.composite) 
                      # difference curves:
                      + s(Time, by = OFCOND) + s(phono.composite, by = OFCOND) 
                      # reference surface:
                      + ti(Time, phono.composite)
                      # difference surface:
                      + ti(Time, phono.composite, by = OFCOND)
                      # random effects:
                      + s(Time, SUBJECT, by = COND, bs = "fs", m = 1),
                      data = data.trg.allCon.start_event,
                      method = "ML", # maximum likelihood estimation
                      weights = 1/weight,
                      AR.start = start.event,
                      rho = itsadug::start_value_rho(gamm.base))

Sys.time()
```

```{r gamm.phono.AR1 table}
# model summary
summary(gamm.phono.AR1)

gamm.phono.AR1.summary <- summary(gamm.phono.AR1)
data.frame(gamm.phono.AR1.summary$p.table)
data.frame(gamm.phono.AR1.summary$s.table)
```

### 3.1. Model Comparison (w/ and w/o Phono Skills as a Predictor)
* including phono skills composite improves model fit
```{r model comparison gamm.base.AR1 and gamm.phono.AR1, echo=TRUE}
# model comparison between gamm.base.AR1 and gamm.phono.AR1
gamm.base.phono.compare <- itsadug::compareML(gamm.base.AR1, gamm.phono.AR1)
gamm.base.phono.compare$table
```

### 3.2. GAMM Visualization
* W1W1-N3W1 increases as phonological skills composite increases (always positive since ranging from 0 to 1.1)
* N3W1-W2W1 decreases as phonological composite increases (from positive to negative, especially towards the very low end of the phono composit scores)
* Phono has no interaction with Time (as shown in the model summary)
```{r visualize, echo=TRUE, fig.show='hold', results='hide'}
# plot diff of COND as a function of phono and time
par_default <- par()
par(cex = 1.5, bg = NA)

plot_diff2(gamm.phono, 
           view = c("Time", "phono.composite"), 
           comp = list(OFCOND = c("W1W1", "N3W1")),
           zlim = c(-2.5,2.5), rm.ranef = TRUE, se = FALSE,
           col = "black", nCol = 1000, labcex = 1.5,
           color = c('#2c7bb6','#abd9e9','#ffffbf','#fdae61','#d7191c'),
           main = "W1W1-N3W1 (Phono)")
plot_diff2(gamm.phono, 
           view = c("Time", "phono.composite"), 
           comp = list(OFCOND = c("N3W1", "W2W1")),
           zlim = c(-2.5,2.5), rm.ranef = TRUE, se = FALSE,
           col = "black", nCol = 1000, labcex = 1.5, nlevels = 5,
           color = c('#2c7bb6','#abd9e9','#ffffbf','#fdae61','#d7191c'),
           main = "N3W1-W2W1 (Lexical)")

par(cex = par_default$cex, bg = par_default$bg)

# http://colorbrewer2.org/?type=diverging&scheme=RdYlBu&n=5
```

```{r visualize GAMM by COND, echo=TRUE, fig.show='hold', results='hide'}
par_default <- par()
par(cex = 1.5, bg = NA)

fvisgam(gamm.phono, 
        view = c("Time", "phono.composite"), 
        cond = list(OFCOND = "W1W1"), 
        zlim = c(-2.5,2.5), rm.ranef = TRUE,
        contour.col = "black", nCol = 1000, labcex = 1.5,
        color = c('#2c7bb6','#abd9e9','#ffffbf','#fdae61','#d7191c'),
        main = "W1W1")
fvisgam(gamm.phono, 
        view = c("Time", "phono.composite"), 
        cond = list(OFCOND = "N3W1"),
        zlim = c(-2.5,2.5), rm.ranef = TRUE,
        contour.col = "black", nCol = 1000, labcex = 1.5,
        color = c('#2c7bb6','#abd9e9','#ffffbf','#fdae61','#d7191c'),
        main = "N3W1")
fvisgam(gamm.phono, 
        view = c("Time", "phono.composite"), 
        cond = list(OFCOND = "W2W1"),
        zlim = c(-2.5,2.5), rm.ranef = TRUE,
        contour.col = "black", nCol = 1000, labcex = 1.5,
        color = c('#2c7bb6','#abd9e9','#ffffbf','#fdae61','#d7191c'),
        main = "W2W1")

par(cex = par_default$cex, bg = par_default$bg)

# http://colorbrewer2.org/?type=diverging&scheme=RdYlBu&n=5
```
